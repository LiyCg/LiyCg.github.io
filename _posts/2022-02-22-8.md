---
title:  "8 Supervised Learning - 8.3-4 KNN / Linear Regression"
categories: 
  - ML_practice
tags:
  - machine learning
  - ML
  - KNN
  - K Nearest Neighbors
  - Linear Regression
toc: true
toc_sticky: true
toc_label: "KNN / Linear Regression"
toc_icon: "blog"
---

## 8. 지도학습 실습 

### 8.3 KNN(K nearest neighbors) 실습



```python
#loading data 
from sklearn import datasets
raw_iris = datasets.load_iris()

#feature, target data 
X = raw_iris.data
y = raw_iris.target

#separating train / test set
from sklearn.model_selection import train_test_split
#random seed를 0으로 설정(다른 숫자 넣어도 무방)
#y_te는 정확도 평가를 위해 추후 ground_truth로 쓰이는 것 
X_tn, X_te, y_tn, y_te = train_test_split(X,y,random_state=0)

#standard scaling the data
from sklearn.preprocessing import StandardScaler
std_scale = StandardScaler()
std_scale.fit(X_tn)
X_tn_std = std_scale.transform(X_tn)
X_te_std = std_scale.transform(X_te)

#training(learning) data
from sklearn.neighbors import KNeighborsClassifier
clf_knn = KNeighborsClassifier(n_neighbors=2)
clf_knn.fit(X_tn_std,y_tn)

#prediction
knn_pred = clf_knn.predict(X_te_std)
print(knn_pred)

#evaluating accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_te, knn_pred)
print(accuracy)

#checking confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_te, knn_pred)
print(cm)

#checking classification report
from sklearn.metrics import classification_report
class_report = classification_report(y_te, knn_pred)
print(class_report)

```

    [2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0
     2]
    0.9473684210526315
    [[13  0  0]
     [ 0 15  1]
     [ 0  1  8]]
                  precision    recall  f1-score   support
    
               0       1.00      1.00      1.00        13
               1       0.94      0.94      0.94        16
               2       0.89      0.89      0.89         9
    
        accuracy                           0.95        38
       macro avg       0.94      0.94      0.94        38
    weighted avg       0.95      0.95      0.95        38
    


### 8.4 선형회귀(Linear Regression) 실습


#### 선형회귀는 기본적인 머신러닝 방법
#### feature data와 target data간의 선형 관계를 파악하는 알고리즘
_________________________________________________
#### y = wx + b (feature 1개라고 가정)
#### y^i = wTxi + b (feature가 n개인 벡터, 텐서라고 가정)
_________________________________________________
#### 인 직선의 방정식을 구하는 문제라고 생각해도됨
### 선형회귀 모델에서 해야 할 일은 feature data x와 target data y를 이용하여 가중치 w를 구하는 것이다. 
#### 위 가중치(wT)의 요소 하나 하나(feature들)이 구하려는 parameter이며, 예측값에 영향을 미친다. 
_________________________________________________
#### <가중치 구하는 법>
#### Least squares estimator를 사용하여 training data로부터 가중치를 구할 수 있다. 
#### 최소제곱법이란 오차의 제곱합이 최소가 되는 추정량을 구하는 방법
#### 최적의 가중치는 최소제곱합을 가중치 W로 미분한 값이 0이되는 가중치 W이다. 
_________________________________________________
#### <제약식이 포함된 회귀 분석>
#### Y? 제약이 없으면 우리가 추정하려는 가중치 W가 폭발적으로 커질 수 있고, 이때문에 분산이 커지는 문제가 발생할 수 있기때문에 (해당 제약식은 라그랑주 승수법으로 풀어냄)
#### 1) Ridge Regression - L2 제약식 사용
#### 2) Lasso Regression - L1 제약식 사용
#### 3) Elastic Net - L1, L2 동시에 적용
#### *자세한 사항은 181pg부터 참고







```python
from sklearn import datasets
raw_boston = datasets.load_boston()

X = raw_boston.data
y = raw_boston.target

from sklearn.model_selection import train_test_split
X_tn, X_te, y_tn, y_te = train_test_split(X,y,random_state=0)

from sklearn.preprocessing import StandardScaler
std_scale = StandardScaler()
std_scale.fit(X_tn)
X_tn_std = std_scale.transform(X_tn)
X_te_std = std_scale.transform(X_te)

#데이터 학습(선형 회귀 분석)
from sklearn.linear_model import LinearRegression
cls_lr = LinearRegression()
cls_lr.fit(X_tn_std,y_tn)

## 여기서 회귀계수, coef는 parameter들을 의미한다. 

# checking linear regression coefficient(추정된 회귀계수)
print(cls_lr.coef_)
# checking scalar terms(추정된 상수항), 선형회귀 그래프에서 y절편을 의미한다
print(cls_lr.intercept_)
```

    [-0.97100092  1.04667838 -0.04044753  0.59408776 -1.80876877  2.60991991
     -0.19823317 -3.00216551  2.08021582 -1.93289037 -2.15743759  0.75199122
     -3.59027047]
    22.6087071240106



```python
#######################
# 데이터 학습(릿지 회귀 분석)# - L2 제약식 사용
#######################


from sklearn.linear_model import Ridge
#알파값은 반드시 양수여야 하고 값이 클수록 강한 제약식을 의미
cls_ridge = Ridge(alpha=1)
cls_ridge.fit(X_tn_std,y_tn)

#릿지 회귀 분석 계수
print(cls_ridge.coef_)
#릿지 회귀 분석 상수항 확인
print(cls_ridge.intercept_)
```

    [-0.96187481  1.02775462 -0.06861144  0.59814087 -1.77318401  2.6205672
     -0.20466821 -2.96504904  2.00091047 -1.85840697 -2.14955893  0.75175979
     -3.57350065]
    22.6087071240106



```python
#######################
# 데이터 학습(라쏘 회귀 분석)# - L1 제약식 사용
#######################

from sklearn.linear_model import Lasso
#릿지 때와 마찬가지로 제약의 정도이다. default는 1이다
cls_lasso = Lasso(alpha=0.01)
cls_lasso.fit(X_tn_std,y_tn)

#라쏘 회귀 분석 계수
print(cls_lasso.coef_)
#라쏘 회귀 분석 상수항 확인
print(cls_lasso.intercept_)
```

    [-0.93949205  1.01037722 -0.05747479  0.59232437 -1.76160385  2.62290366
     -0.17911018 -2.92328686  1.93398258 -1.81118512 -2.14705184  0.73964238
     -3.59732302]
    22.6087071240106



```python
#######################
# 데이터 학습(엘라스틱넷 회귀 분석)#
#######################

from sklearn.linear_model import ElasticNet
#l1_ratio는 L1제약식의 비율이다. l1_ratio가 1이라면 l2 제약식은 사용하지 않는다는 의미
cls_en = ElasticNet(alpha=0.01, l1_ratio=0.01)
cls_en.fit(X_tn_std,y_tn)

#라쏘 회귀 분석 계수
print(cls_en.coef_)
#라쏘 회귀 분석 상수항 확인
print(cls_en.intercept_)
```

    [-0.93905353  0.98057259 -0.13487248  0.60767135 -1.68277217  2.64642801
     -0.21978011 -2.86727682  1.80930385 -1.68204979 -2.12935659  0.75122826
     -3.52939748]
    22.6087071240106



```python
#######################
# 데이터 예측 #
#######################

#기본 linear regression
pred_lr = cls_lr.predict(X_te_std)
#ridge
pred_ridge = cls_ridge.predict(X_te_std)
#lasso
pred_lasso = cls_lasso.predict(X_te_std)
#elastic net
pred_en = cls_en.predict(X_te_std)
```


```python
#######################
# Model performance Eval #
#######################

#r2_score of default / ridge / lasso / elastic net 's performance
#0-1사잇값을 가지고 높을수록 좋은 성능
from sklearn.metrics import r2_score
print(r2_score(y_te,pred_lr))
print(r2_score(y_te,pred_ridge))
print(r2_score(y_te,pred_lasso))
print(r2_score(y_te,pred_en))

#MSE of default / ridge / lasso / elastic net 's performance 
#작을수록 좋은 성능
from sklearn.metrics import mean_squared_error
print(mean_squared_error(y_te,pred_lr))
print(mean_squared_error(y_te,pred_ridge))
print(mean_squared_error(y_te,pred_lasso))
print(mean_squared_error(y_te,pred_en))

```

    0.635463843320213
    0.6345884564889054
    0.6343061000666704
    0.6322273400977834
    29.782245092302357
    29.853763334547608
    29.876831576246808
    30.046664219036877

